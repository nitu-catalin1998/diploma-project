{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Licenta.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_EV7Tn1MI4z",
        "colab_type": "text"
      },
      "source": [
        "# Intelligent Linguistic System for the Grammar of the Romanian Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drsFhXD9Qyyf",
        "colab_type": "text"
      },
      "source": [
        "## Authors:\n",
        "\n",
        "* Conf. Dr. Ing. Rebedea Traian-Eugen (Scientific coordinator)\n",
        "* Ing. Coteț Teodor-Mihai (Special thanks - Co-tutor)\n",
        "* Nițu Ioan-Florin-Cătălin 342C4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pOy_RwmQIzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################\n",
        "##                           _    _   _ __    _ __                            ##\n",
        "##                          | |  | | | '_ \\  | '_ \\                           ##\n",
        "##                          | |  | | | |_) | | |_) |                          ##\n",
        "##                          | |  | | | .__/  | '_ )                           ##\n",
        "##                          | \\__/ | | |     | |_) |                          ##\n",
        "##                          \\______/ |_|     |_.__/                           ##\n",
        "##                          -------- _______ -------                          ##\n",
        "##                                                                            ##\n",
        "################################################################################\n",
        "##  File:           Licenta.ipynb                                             ##\n",
        "##  Description:    Intelligent Linguistic System for Romanian Grammar        ##\n",
        "##  By:             ioan_florin.nitu                                          ##\n",
        "################################################################################\n",
        "##  This program implements a program for the Romanian Language Grammar       ##\n",
        "################################################################################\n",
        "##  The program operates with Neural Networks:                                ##\n",
        "##   - Datasets                                                               ##\n",
        "##   - Attention                                                              ##\n",
        "##   - Layers                                                                 ##\n",
        "##   - Encoders and Decoders                                                  ##\n",
        "##   - Transformers                                                           ##\n",
        "##  Finished in 2 weeks of continue work!                                     ##\n",
        "################################################################################\n",
        "##   _______________________________________                                  ##\n",
        "##  / Look for my W-USO ;)                  \\                                 ##\n",
        "##  \\ (No, seriously, don't look...)        /                                 ##\n",
        "##   ---------------------------------------                                  ##\n",
        "##          \\   ^__^                                                          ##\n",
        "##           \\  (oo)\\_______                                                  ##\n",
        "##              (__)\\       )\\/\\                                              ##\n",
        "##                  ||----w |                                                 ##\n",
        "##                  ||     ||                                                 ##\n",
        "##                                                                            ##\n",
        "##                                                                            ##\n",
        "################################################################################\n",
        "\n",
        "### LICENTA\n",
        " #\n",
        " # Licenta.ipynb\n",
        " #\n",
        " # @author Nitu Ioan Florin Catalin\n",
        " # @group 342C4\n",
        " # @version 1\n",
        " # @since Rock 'n' Roll\n",
        " #\n",
        " # Copyright (c) 2020 ACS\n",
        " #\n",
        " ##\n",
        "\n",
        "\"LICENTA: Nitu Ioan-Florin-Catalin, 342C4, CTI, DC, FAC, UPB, 2020\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLR8BHY4yFFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from nltk.translate.bleu_score import SmoothingFunction, \\\n",
        "                                      sentence_bleu as bleu_score\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIIKY3U2Hwko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J70cQImRgTZB",
        "colab_type": "text"
      },
      "source": [
        "## Setup Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoweMTuJK8ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "MAX_LENGTH = 512\n",
        "EPOCHS = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fy4z980M5dn",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES2sqJK3Myls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(filename, directory=\"/content/drive/My Drive\"):\n",
        "    def spliter(example):\n",
        "        sentences = tf.strings.split(example, sep='\\t')\n",
        "        (tar, inp) = (sentences[0], sentences[1])\n",
        "        return (inp, tar)\n",
        "    dataset = tf.data.TextLineDataset(os.path.join(directory, filename))\n",
        "    splited_dataset = dataset.map(lambda example: spliter(example))\n",
        "    return splited_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu8AvErxRW0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#huge_examples = load_dataset(\"10_mil_dirty_clean_better.txt\")\n",
        "#huge_examples = load_dataset(\"1_mil_dirty_clean_better.txt\")\n",
        "#huge_examples = load_dataset(\"50_k_dirty_clean_better.txt\")\n",
        "train_examples = load_dataset(\"train.txt\")\n",
        "test_examples = load_dataset(\"test.txt\")\n",
        "eval_examples = load_dataset(\"dev.txt\")\n",
        "#train_examples = huge_examples.concatenate(train_examples)\n",
        "tokenizer_inp = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "                    (inp.numpy() for (inp, tar) in train_examples),\n",
        "                    target_vocab_size=2 ** 11)\n",
        "tokenizer_tar = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "                    (tar.numpy() for (inp, tar) in train_examples),\n",
        "                    target_vocab_size=2 ** 11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbT491LsTUs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_encode(inp, tar):\n",
        "    def encode(inp, tar):\n",
        "        inp = [tokenizer_inp.vocab_size] + tokenizer_inp.encode(inp.numpy()) + \\\n",
        "                [tokenizer_inp.vocab_size + 1]\n",
        "        tar = [tokenizer_tar.vocab_size] + tokenizer_tar.encode(tar.numpy()) + \\\n",
        "                [tokenizer_tar.vocab_size + 1]\n",
        "        return (inp, tar)\n",
        "    results = tf.py_function(encode, [inp, tar], [tf.int64, tf.int64])\n",
        "    (result_inp, result_tar) = results\n",
        "    result_inp.set_shape([None])\n",
        "    result_tar.set_shape([None])\n",
        "    return (result_inp, result_tar)\n",
        "\n",
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "    return tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nggq0OZNx5XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "eval_dataset = eval_examples.map(tf_encode)\n",
        "eval_dataset = eval_dataset.filter(filter_max_length)\n",
        "eval_dataset = eval_dataset.padded_batch(BATCH_SIZE)\n",
        "test_dataset = test_examples.map(tf_encode)\n",
        "test_dataset = test_dataset.filter(filter_max_length)\n",
        "test_dataset = test_dataset.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGK1VCfHhGeA",
        "colab_type": "text"
      },
      "source": [
        "### Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxL3E4QehH_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeMKHR6ctG91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47W7ixe4KG8p",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b9eEmAGhV-4",
        "colab_type": "text"
      },
      "source": [
        "### Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhDTOJlKhWJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "                        # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtBh_JhQiFQ5",
        "colab_type": "text"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyIs4MLWiKdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is\n",
        "        (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "            # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "            # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "            # (batch_size, num_heads, seq_len_v, depth)\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q,\n",
        "        #                             seq_len_k)\n",
        "        (scaled_attention, attention_weights) = scaled_dot_product_attention(q,\n",
        "                                                                    k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "                           # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "                           # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "                 # (batch_size, seq_len_q, d_model)\n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l21AYJHWnpyV",
        "colab_type": "text"
      },
      "source": [
        "## Coder Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfiEksjxnUXU",
        "colab_type": "text"
      },
      "source": [
        "### Point Wise Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4Kn4pURnUi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "                                tf.keras.layers.Dense(dff, activation=\"relu\"),\n",
        "                                # (batch_size, seq_len, dff)\n",
        "                                tf.keras.layers.Dense(d_model)\n",
        "                                # (batch_size, seq_len, d_model)\n",
        "                               ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twyb1H0snwrp",
        "colab_type": "text"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmOtKmAAnqFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        (attn_output, _) = self.mha(x, x, x, mask)\n",
        "                           # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "               # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "               # (batch_size, input_seq_len, d_model)\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeyPkLv2oHcw",
        "colab_type": "text"
      },
      "source": [
        "### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifSNYcs7oHlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        (attn1, attn_weights_block1) = self.mha1(x, x, x, look_ahead_mask)\n",
        "                                       # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        (attn2, attn_weights_block2) = self.mha2(enc_output, enc_output, out1,\n",
        "                                                 padding_mask)\n",
        "                                       # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "               # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "               # (batch_size, target_seq_len, d_model)\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-ZVXYiyprTb",
        "colab_type": "text"
      },
      "source": [
        "## Coder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnVdmNFkgjX4",
        "colab_type": "text"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c2iLrmMgo-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    def get_angles(pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :], d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvpYLpaJpw1E",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFpk7bMApxct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                                d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3pre7N6px1I",
        "colab_type": "text"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIoGUsaepyNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                                d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                   look_ahead_mask,\n",
        "                                                   padding_mask)\n",
        "            attention_weights[\"decoder_layer{}_block1\".format(i + 1)] = block1\n",
        "            attention_weights[\"decoder_layer{}_block2\".format(i + 1)] = block2\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWgogNN2qtWJ",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkhhs3XWqtFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                               input_vocab_size, pe_input, rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                               target_vocab_size, pe_target, rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask,\n",
        "             dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "                     # (batch_size, inp_seq_len, d_model)\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training,\n",
        "                                                     look_ahead_mask,\n",
        "                                                     dec_padding_mask)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "                       # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JiTGxzrrxDH",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kai_Pph9rw7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The values used in the base model of transformer were;\n",
        "# num_layers = 6, d_model = 512, dff = 2048.\n",
        "# See the paper for all the other versions of the transformer.\n",
        "num_layers = 2\n",
        "d_model = 128\n",
        "dff = 128\n",
        "num_heads = 2\n",
        "\n",
        "input_vocab_size = tokenizer_inp.vocab_size + 2\n",
        "target_vocab_size = tokenizer_tar.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOkBHp92r10s",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shitq6lsr2Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_ubuW8GsBwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KygjqWUsNfB",
        "colab_type": "text"
      },
      "source": [
        "## Loss and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K3b9P8BsNXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "                                                        name=\"train_accuracy\")\n",
        "test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "                                                        name=\"test_accuracy\")\n",
        "eval_loss = tf.keras.metrics.Mean(name=\"eval_loss\")\n",
        "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"eval_accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xSaDgM3shjD",
        "colab_type": "text"
      },
      "source": [
        "## Training and Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jyDeIOcsf7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                          target_vocab_size, pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size, rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1GbZ3TDIYPV",
        "colab_type": "text"
      },
      "source": [
        "### Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5vuwceqtG7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print (\"Latest checkpoint restored!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQiDuQ1uIdN6",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDNndiagJ68s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_log_dir = 'logs/gradient_tape/train'\n",
        "test_log_dir = 'logs/gradient_tape/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
        "%tensorboard --logdir logs/gradient_tape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0EIX8U2tUGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "                        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "                        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "                       ]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    (enc_padding_mask, combined_mask, dec_padding_mask) = create_masks(inp,\n",
        "                                                                       tar_inp)\n",
        "    with tf.GradientTape() as tape:\n",
        "        (predictions, _) = transformer(inp, tar_inp, True, enc_padding_mask,\n",
        "                                       combined_mask, dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "        optimizer.apply_gradients(zip(gradients,\n",
        "                                      transformer.trainable_variables))\n",
        "        train_loss(loss)\n",
        "        train_accuracy(tar_real, predictions)\n",
        "\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "test_step_signature = [\n",
        "                       tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "                       tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "                      ]\n",
        "\n",
        "@tf.function(input_signature=test_step_signature)\n",
        "def test_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    (enc_padding_mask, combined_mask, dec_padding_mask) = create_masks(inp,\n",
        "                                                                       tar_inp)\n",
        "    with tf.GradientTape() as tape:\n",
        "        (predictions, _) = transformer(inp, tar_inp, False, enc_padding_mask,\n",
        "                                       combined_mask, dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        test_loss(loss)\n",
        "        test_accuracy(tar_real, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuMjI4iUtvR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    # inp -> incorrect, tar -> correct\n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                    epoch + 1, batch, train_loss.result(),\n",
        "                    train_accuracy.result()))\n",
        "    with train_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(\"Saving checkpoint for epoch {} at {}\".format(epoch + 1,\n",
        "                                                            ckpt_save_path))\n",
        "    print(\"Epoch {} Loss {:.4f} Accuracy {:.4f}\".format(epoch + 1,\n",
        "                                                        train_loss.result(),\n",
        "                                                        train_accuracy.result())\n",
        "    )\n",
        "    for (batch, (inp, tar)) in enumerate(test_dataset):\n",
        "        test_step(inp, tar)\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Test {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                    epoch + 1, batch, test_loss.result(),\n",
        "                    test_accuracy.result()))\n",
        "    with test_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    print(\"Test {} Loss {:.4f} Accuracy {:.4f}\".format(epoch + 1,\n",
        "                                                       test_loss.result(),\n",
        "                                                       test_accuracy.result())\n",
        "    )\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNeknOcgt_i4",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VmfgxwYX1VeJ",
        "colab": {}
      },
      "source": [
        "# The @tf.function trace-compiles eval_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "eval_step_signature = [\n",
        "                       tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "                       tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "                      ]\n",
        "\n",
        "@tf.function(input_signature=eval_step_signature)\n",
        "def eval_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    (enc_padding_mask, combined_mask, dec_padding_mask) = create_masks(inp,\n",
        "                                                                       tar_inp)\n",
        "    with tf.GradientTape() as tape:\n",
        "        (predictions, _) = transformer(inp, tar_inp, False, enc_padding_mask,\n",
        "                                       combined_mask, dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        eval_loss(loss)\n",
        "        eval_accuracy(tar_real, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "inA2CwWg1Vej",
        "colab": {}
      },
      "source": [
        "for epoch in range(1):\n",
        "    start = time.time()\n",
        "    eval_loss.reset_states()\n",
        "    eval_accuracy.reset_states()\n",
        "    # inp -> incorrect, tar -> correct\n",
        "    for (batch, (inp, tar)) in enumerate(eval_dataset):\n",
        "        eval_step(inp, tar)\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                    epoch, batch, eval_loss.result(),\n",
        "                    eval_accuracy.result()))\n",
        "    print(\"Epoch {} Loss {:.4f} Accuracy {:.4f}\".format(epoch,\n",
        "                                                        eval_loss.result(),\n",
        "                                                        eval_accuracy.result())\n",
        "    )\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p_Kvqj-Jay_",
        "colab_type": "text"
      },
      "source": [
        "### Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX_vZZdcuCUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    start_token = [tokenizer_inp.vocab_size]\n",
        "    end_token = [tokenizer_inp.vocab_size + 1]\n",
        "    # inp sentence is incorrect, hence adding the start and end token\n",
        "    inp_sentence = start_token + tokenizer_inp.encode(inp_sentence) + end_token\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "    # as the target is correct, the first word to the transformer should be the\n",
        "    # correct start token.\n",
        "    decoder_input = [tokenizer_tar.vocab_size]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    for i in range(MAX_LENGTH):\n",
        "        (enc_padding_mask, combined_mask, dec_padding_mask) = create_masks(\n",
        "                                                          encoder_input, output)\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        (predictions, attention_weights) = transformer(encoder_input, output,\n",
        "                                                       False, enc_padding_mask,\n",
        "                                                       combined_mask,\n",
        "                                                       dec_padding_mask)\n",
        "        # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == tokenizer_tar.vocab_size + 1:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "        # concatentate the predicted_id to the output which is given to the\n",
        "        # decoder as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFpDyAFkuDxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "    sentence = tokenizer_inp.encode(sentence)\n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "        # plot the attention weights\n",
        "        ax.matshow(attention[head][:-1, :], cmap=\"viridis\")\n",
        "        fontdict = {\"fontsize\": 10}\n",
        "        ax.set_xticks(range(len(sentence) + 2))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "        ax.set_ylim(len(result) - 1.5, -0.5)\n",
        "        ax.set_xticklabels([\"<start>\"] + \\\n",
        "                           [tokenizer_inp.decode([i]) for i in sentence] + \\\n",
        "                           [\"<end>\"],\n",
        "                           fontdict=fontdict, rotation=90)\n",
        "        ax.set_yticklabels([tokenizer_tar.decode([i]) \\\n",
        "                            for i in result if i < tokenizer_tar.vocab_size],\n",
        "                           fontdict=fontdict)\n",
        "        ax.set_xlabel(\"Head {}\".format(head + 1))\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RviVmy0vuE0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct(sentence, real_sentence=None, verbose=True, plot=\"\"):\n",
        "    score = None\n",
        "    assert isinstance(sentence, str) == True\n",
        "    (result, attention_weights) = evaluate(sentence)\n",
        "    predicted_sentence = tokenizer_tar.decode([i\n",
        "                                               for i in result\n",
        "                                               if i < tokenizer_tar.vocab_size])\n",
        "    if verbose:\n",
        "        print(\"Input: {}\".format(sentence))\n",
        "        print(\"Predicted correction: {}\".format(predicted_sentence))\n",
        "    if not real_sentence is None:\n",
        "        assert isinstance(real_sentence, str) == True\n",
        "        try:\n",
        "            score = bleu_score([real_sentence.split(' ')],\n",
        "                               predicted_sentence.split(' '),\n",
        "                               smoothing_function=SmoothingFunction().method4)\n",
        "        except:\n",
        "            score = 0\n",
        "        if verbose:\n",
        "            print(\"Real correction: {}\".format(real_sentence))\n",
        "            print(\"BLEU score: {:.4f}\".format(score))\n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1SMd1rgJeaI",
        "colab_type": "text"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD9kIqmsuHeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct(\"Alte mărci comerciale utilizate vreme îndelungată sunt Löwenbräu, deținătorii căreia spun că este folosită din 1383, și Stella Artois  din 1366.\",\n",
        "        real_sentence=\"Alte mărci comerciale utilizate vreme îndelungată sunt Löwenbräu, deținătorii căreia spun că este folosită din 1383, și Stella Artois  din 1366.\",\n",
        "        plot=\"decoder_layer2_block2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWgV6RG4uHPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct(\"Cea mai importantă este ceea surprinsă asupra luni Noiembrie.\",\n",
        "        real_sentence=\"Cea mai importantă este cea surprinsă asupra lunii Noiembrie.\",\n",
        "        plot=\"decoder_layer2_block2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLGWAVpluHIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct(\"„Nici odată nam văzut cartea așa” a mărturisit el.\",\n",
        "        real_sentence=\"„Niciodată n-am văzut cartea așa”, a mărturisit el.\",\n",
        "        plot=\"decoder_layer2_block2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtnis-P-CqnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct(\"Incinta exterioară în partea de est, astăzi goală, deținea altă dată, donjonul construit pentru Filip al II-lea  în 1219, cu ocazia unei confiscări a senioriei, ca Catedrala Colegială Saint-Ythier, mutată de Maximilien De Béthune  în interiorul orașului.\",\n",
        "        real_sentence=\"Incinta exterioară în partea de est, astăzi goală, deținea altădată, donjonul construit pentru Filip al II-lea  în 1219, cu ocazia unei confiscări a senioriei, ca și Catedrala Colegială Saint-Ythier, mutată de Maximilien De Béthune  în interiorul orașului.\",\n",
        "        plot=\"decoder_layer2_block2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZfsb7VquG_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can pass different layers and attention blocks of the decoder to the `plot` parameter.\n",
        "correct(\"În prezent, satul are 3.897 locuitori, prepoderent ucraineni.\",\n",
        "        real_sentence=\"În prezent, satul are 3.897 locuitori, preponderent ucraineni.\",\n",
        "        plot=\"decoder_layer2_block2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSQpMYJwDCC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = 0.0\n",
        "number = 0\n",
        "for (inp, tar) in test_examples:\n",
        "    score = correct(str(inp.numpy()), real_sentence=str(tar.numpy()),\n",
        "                    verbose=False)\n",
        "    print(score)\n",
        "    if score is None:\n",
        "        score = 0\n",
        "    scores += score\n",
        "    number += 1\n",
        "print(f\"BLEU score: {scores / number}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKI1c63EuNXy",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeVEK5F3RNWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "!zip -r /content/model.zip /content/checkpoints/train /content/logs\n",
        "files.download(\"/content/model.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GKvDnmrOf8Y",
        "colab_type": "text"
      },
      "source": [
        "All good!"
      ]
    }
  ]
}